{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Nadam, SGD, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "\n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_chars\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all training files= ['data/names/Vietnamese.txt', 'data/names/Czech.txt', 'data/names/Spanish.txt', 'data/names/Arabic.txt', 'data/names/Irish.txt', 'data/names/Scottish.txt', 'data/names/Dutch.txt', 'data/names/French.txt', 'data/names/Italian.txt', 'data/names/Greek.txt', 'data/names/Korean.txt', 'data/names/Japanese.txt', 'data/names/Polish.txt', 'data/names/Chinese.txt', 'data/names/German.txt', 'data/names/Russian.txt', 'data/names/Portuguese.txt', 'data/names/English.txt']\n",
      "Slusarski\n",
      "Total  20074 names across 18 categories\n"
     ]
    }
   ],
   "source": [
    "print('all training files=',findFiles('data/names/*.txt'))\n",
    "\n",
    "pad_char = '#'\n",
    "all_chars = string.ascii_letters + \" .,;'\" + pad_char\n",
    "n_chars = len(all_chars)\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_names dictionary, a list of names per language\n",
    "category_names_dict = {}\n",
    "all_categories = []\n",
    "\n",
    "name_counts = []\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    names = readLines(filename)\n",
    "    category_names_dict[category] = names\n",
    "    name_counts.append(len(names))\n",
    "\n",
    "num_samples = np.sum(name_counts)\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print('Total ',num_samples,'names across',n_categories,'categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of characters, this is the encoding dimension of each character in a name :  58\n"
     ]
    }
   ],
   "source": [
    "print('No of characters, this is the encoding dimension of each character in a name : ',n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_categories= ['Vietnamese', 'Czech', 'Spanish', 'Arabic', 'Irish', 'Scottish', 'Dutch', 'French', 'Italian', 'Greek', 'Korean', 'Japanese', 'Polish', 'Chinese', 'German', 'Russian', 'Portuguese', 'English'] \n",
      "\n",
      "category \"English\" has  3668 names\n",
      "['Abbas', 'Abbey', 'Abbott', 'Abdi', 'Abel']\n"
     ]
    }
   ],
   "source": [
    "print('all_categories=',all_categories,'\\n')\n",
    "print('category \"English\" has ',len(category_names_dict['English']),'names')\n",
    "print(category_names_dict['English'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese 73\n",
      "Czech 519\n",
      "Spanish 298\n",
      "Arabic 2000\n",
      "Irish 232\n",
      "Scottish 100\n",
      "Dutch 297\n",
      "French 277\n",
      "Italian 709\n",
      "Greek 203\n",
      "Korean 94\n",
      "Japanese 991\n",
      "Polish 139\n",
      "Chinese 268\n",
      "German 724\n",
      "Russian 9408\n",
      "Portuguese 74\n",
      "English 3668\n"
     ]
    }
   ],
   "source": [
    "labels = list(category_names_dict.keys())\n",
    "values = [len(names) for names in category_names_dict.values()]\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "centers = range(len(values))\n",
    "plt.bar(centers, values, align='center', tick_label=labels)\n",
    "plt.show()\n",
    "\n",
    "for i,ll in enumerate(labels):\n",
    "    print(ll,values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Vietnamese]\n",
      "Vietnamese\n",
      "True\n",
      "Adding  Vietnamese 73\n",
      "73\n",
      "---------------\n",
      "[Czech]\n",
      "Czech\n",
      "True\n",
      "Adding  Czech 519\n",
      "592\n",
      "---------------\n",
      "[Spanish]\n",
      "Spanish\n",
      "True\n",
      "Adding  Spanish 298\n",
      "890\n",
      "---------------\n",
      "[Arabic]\n",
      "Arabic\n",
      "True\n",
      "Adding  Arabic 2000\n",
      "2890\n",
      "---------------\n",
      "[Irish]\n",
      "Irish\n",
      "True\n",
      "Adding  Irish 232\n",
      "3122\n",
      "---------------\n",
      "[Scottish]\n",
      "Scottish\n",
      "True\n",
      "Adding  Scottish 100\n",
      "3222\n",
      "---------------\n",
      "[Dutch]\n",
      "Dutch\n",
      "True\n",
      "Adding  Dutch 297\n",
      "3519\n",
      "---------------\n",
      "[French]\n",
      "French\n",
      "True\n",
      "Adding  French 277\n",
      "3796\n",
      "---------------\n",
      "[Italian]\n",
      "Italian\n",
      "True\n",
      "Adding  Italian 709\n",
      "4505\n",
      "---------------\n",
      "[Greek]\n",
      "Greek\n",
      "True\n",
      "Adding  Greek 203\n",
      "4708\n",
      "---------------\n",
      "[Korean]\n",
      "Korean\n",
      "True\n",
      "Adding  Korean 94\n",
      "4802\n",
      "---------------\n",
      "[Japanese]\n",
      "Japanese\n",
      "True\n",
      "Adding  Japanese 991\n",
      "5793\n",
      "---------------\n",
      "[Polish]\n",
      "Polish\n",
      "True\n",
      "Adding  Polish 139\n",
      "5932\n",
      "---------------\n",
      "[Chinese]\n",
      "Chinese\n",
      "True\n",
      "Adding  Chinese 268\n",
      "6200\n",
      "---------------\n",
      "[German]\n",
      "German\n",
      "True\n",
      "Adding  German 724\n",
      "6924\n",
      "---------------\n",
      "[Russian]\n",
      "skip\n",
      "------------------\n",
      "[Portuguese]\n",
      "Portuguese\n",
      "True\n",
      "Adding  Portuguese 74\n",
      "6998\n",
      "---------------\n",
      "[English]\n",
      "skip\n",
      "------------------\n",
      "[Other]\n",
      "skip\n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian 9408\n",
      "English 3668\n",
      "Other 6998\n"
     ]
    }
   ],
   "source": [
    "category_names_dict['Other'] = []\n",
    "to_skip = ['English','Russian','Other']\n",
    "for k,v in category_names_dict.items():\n",
    "    print('['+k,']',sep='')\n",
    "    k = k.strip()\n",
    "    if k not in to_skip:\n",
    "        print(k)\n",
    "        print(k is not 'English' and k is not 'Russian')\n",
    "        print('Adding ',k,len(v))\n",
    "        category_names_dict['Other'].extend(v)\n",
    "        print(len(category_names_dict['Other']))\n",
    "        print('---------------')\n",
    "    else:\n",
    "        print('skip')\n",
    "        print('------------------')\n",
    "        \n",
    "category_names_dict = {i:category_names_dict[i] for i in category_names_dict if i in to_skip}\n",
    "\n",
    "labels = list(category_names_dict.keys())\n",
    "values = [len(names) for names in category_names_dict.values()]\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "centers = range(len(values))\n",
    "plt.bar(centers, values, align='center', tick_label=labels)\n",
    "plt.show()\n",
    "\n",
    "for i,ll in enumerate(labels):\n",
    "    print(ll,values[i])\n",
    "    \n",
    "all_categories = list(category_names_dict.keys())\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Russian', 'English', 'Other']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(all_categories)\n",
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J= (1, 58)\n",
      "Jones= (5, 1, 58)\n",
      "\n",
      " pad_char= # [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Find char index from all_chars, e.g. \"a\" = 0\n",
    "def charToIndex(char):\n",
    "    return all_chars.find(char)\n",
    "\n",
    "# Just for demonstration, turn a char into a <1 x n_chars> Tensor\n",
    "def charToTensor_one_hot(char):\n",
    "    tensor = np.zeros((1, n_chars))\n",
    "    tensor[0][charToIndex(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "def charToTensor(char):\n",
    "    tensor = np.zeros(1,dtype=np.long)\n",
    "    tensor[0] = charToIndex(char)\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_chars>,\n",
    "# or an array of one-hot char vectors\n",
    "def seqToTensor_one_hot(seq):\n",
    "    tensor = np.zeros((len(seq),1, n_chars))\n",
    "    for idx, char in enumerate(seq):\n",
    "        tensor[idx][0][charToIndex(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "def seqToTensor(seq):\n",
    "    tensor = np.zeros(len(seq), dtype=np.long)\n",
    "    for idx, char in enumerate(seq):\n",
    "        tensor[idx] = int(charToIndex(char))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "print('J=',charToTensor_one_hot('J').shape)\n",
    "\n",
    "print('Jones=',seqToTensor_one_hot('Jones').shape)\n",
    "\n",
    "print('\\n pad_char=',pad_char,charToTensor_one_hot(pad_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_generator(all_categories, category_names_dict, batch_size, n_chars, pad_char, shuffle):\n",
    "    \n",
    "    #flatten the dictionary to a list of tuples.\n",
    "    dict_tuples = []\n",
    "    for category,names in category_names_dict.items():\n",
    "        for nm in names:\n",
    "            dict_tuples.append((category,nm))\n",
    "    \n",
    "    num_samples = len(dict_tuples)\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    print('batch_data_generator: num_samples =',num_samples,'num_batches = ',num_batches)\n",
    "    \n",
    "    epoch_num = 0\n",
    "    n_cat = len(category_names_dict)\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        if shuffle:\n",
    "            random.shuffle(dict_tuples)\n",
    "            random.shuffle(dict_tuples)\n",
    "        indices = np.arange(num_samples)\n",
    "        \n",
    "        for batch_id in range(num_batches): #for each batch of names\n",
    "            batch_indices = indices[batch_id * batch_size : (batch_id + 1) * batch_size]\n",
    "\n",
    "            max_seqlen = 0 #max length of names in a batch.\n",
    "            batch_names = []\n",
    "            batch_labels = []\n",
    "            batch_categories = []    \n",
    "            name_tensors = []\n",
    "            \n",
    "            for b_ind in batch_indices:\n",
    "                a_name = dict_tuples[b_ind][1]            #'Alex'\n",
    "                category = dict_tuples[b_ind][0]          #'English'\n",
    "                label = all_categories.index(category)    #17\n",
    "                name_tensor = seqToTensor_one_hot(a_name)\n",
    "                max_seqlen = name_tensor.shape[0] if name_tensor.shape[0] >= max_seqlen else max_seqlen\n",
    "                batch_names.append(a_name)\n",
    "                batch_labels.append(label)\n",
    "                batch_categories.append(category)\n",
    "                name_tensors.append(np.squeeze(name_tensor))\n",
    "\n",
    "            #for nt in name_tensors:    \n",
    "            #    print('name_tensor=',nt.shape)\n",
    "                \n",
    "            #convert the batch list of tuples to tensors.\n",
    "            #Put all the selected names into a single tensor for input to RNN          \n",
    "            pad_char_tensor = charToTensor_one_hot(pad_char) #tensor corresponding to pad_char            \n",
    "            #create a tensor of size [batch_size x max_seqlen x n_char] filled with pad_char\n",
    "            batch_names_tensor = np.broadcast_to(\n",
    "                pad_char_tensor,(batch_size, max_seqlen, pad_char_tensor.shape[1])\n",
    "                ).copy()\n",
    "            \n",
    "            #print('batch_names_tensor',batch_names_tensor.shape)\n",
    "            \n",
    "            for i,name_tensor in enumerate(name_tensors):\n",
    "                num_chars = name_tensor.shape[0]\n",
    "                #print(num_chars,'assigning',name_tensor.shape,'to',batch_names_tensor[i,0:num_chars,:].shape)\n",
    "                batch_names_tensor[i,-num_chars:,:] = name_tensor #Left padding is done with pad_char\n",
    "            \n",
    "            batch_names_tensor = np.array(np.squeeze(batch_names_tensor))\n",
    "            \n",
    "            batch_labels_one_hot = np.array(to_categorical(batch_labels, num_classes=n_cat))\n",
    "            \n",
    "            yield(batch_names_tensor, batch_labels_one_hot)\n",
    "            \n",
    "        #done looping through all batches.\n",
    "        #go to the top and permute the file indices.\n",
    "        epoch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of name categories, this is the no. of output categories =  3\n"
     ]
    }
   ],
   "source": [
    "print('Number of name categories, this is the no. of output categories = ',n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 1 batch of data to test Keras model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_data_generator: num_samples = 20074 num_batches =  4014\n",
      "batch_names_tensor (5, 10, 58)\n",
      "batch_labels_tensor_one_hot (5, 3)\n",
      "\n",
      "batch_labels_tensor_one_hot =\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 4\n",
    "batch_size = 5\n",
    "shuffle = True\n",
    "\n",
    "batch_generator = batch_data_generator(all_categories, category_names_dict, batch_size, n_chars, pad_char, shuffle)\n",
    "batch_names_tensor, batch_labels_tensor_one_hot = next(batch_generator)\n",
    "\n",
    "print(\"batch_names_tensor\",batch_names_tensor.shape)\n",
    "print(\"batch_labels_tensor_one_hot\",batch_labels_tensor_one_hot.shape)\n",
    "\n",
    "print(\"\\nbatch_labels_tensor_one_hot =\\n\",batch_labels_tensor_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A way to use Keras to build a model for character level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Model summary--------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 58)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 4)                 1008      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 1,023\n",
      "Trainable params: 1,023\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "y_pred = \n",
      " [[0.3196215  0.3282915  0.35208696]\n",
      " [0.3212379  0.32976478 0.3489973 ]\n",
      " [0.32440522 0.34431443 0.33128032]\n",
      " [0.35378945 0.349261   0.29694957]\n",
      " [0.30627587 0.33516407 0.35856003]]\n"
     ]
    }
   ],
   "source": [
    "def build_model_1(n_hidden, n_chars, n_categories):\n",
    "    inputs = Input(shape=(None, n_chars)) #n_chars = feature size\n",
    "    lstm = LSTM(n_hidden)(inputs)\n",
    "    dense = Dense(n_categories, activation='softmax')(lstm)\n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #initialize hidden states, not sure if the initialization works.\n",
    "    #layer[1] is LSTM\n",
    "    hidden_states = K.variable(value=np.zeros([1, n_hidden]))\n",
    "    cell_states = K.variable(value=np.zeros([1, n_hidden]))\n",
    "    model.layers[1].states[0] = hidden_states\n",
    "    model.layers[1].states[1] = cell_states \n",
    "    \n",
    "    # This initialization also compiles without errors. \n",
    "    #c_0 = tf.convert_to_tensor(np.zeros([1, n_hidden]).astype(np.float32))\n",
    "    #h_0 = tf.convert_to_tensor(np.zeros([1, n_hidden]).astype(np.float32))\n",
    "    #model.layers[1].states[0] = h_0\n",
    "    #model.layers[1].states[1] = c_0\n",
    "\n",
    "    print('--------------Model summary--------------')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model1 = build_model_1(n_hidden, n_chars, n_categories)\n",
    "X_input = tf.placeholder(tf.float32, shape=(None, None, n_chars))\n",
    "y_output = model1(X_input)\n",
    "\n",
    "X = batch_names_tensor\n",
    "y_true = batch_labels_tensor_one_hot\n",
    "\n",
    "y_pred = model1.predict(X)\n",
    "print('y_pred = \\n',y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print predictions and accuracy comparing with true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred      =  [2 2 1 0 2]\n",
      "true_labels =  [0 2 1 2 1]\n",
      "Accuracy =  [0. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "sess = K.get_session()\n",
    "top_n, top_i = tf.nn.top_k(y_pred, k=1)\n",
    "top_values, top_indices = sess.run(tf.nn.top_k(y_pred, k=1))\n",
    "print('y_pred      = ',np.squeeze(top_indices))\n",
    "\n",
    "batch_labels_tensor = np.argmax(batch_labels_tensor_one_hot,axis=1)\n",
    "print('true_labels = ',batch_labels_tensor)\n",
    "\n",
    "#Calculate accuracy of each prediction using Keras metrics\n",
    "metric = tf.keras.metrics.categorical_accuracy(y_true,y_pred)\n",
    "print('Accuracy = ',sess.run(metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to use Keras to build a model for character level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel2 = build_model_2(n_hidden, n_chars, n_categories)\\nmodel2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n\\nX_input = tf.placeholder(tf.float32, shape=(None, None, n_chars))\\ny_output = model2(X_input)\\n\\nX = batch_names_tensor\\ny_true = batch_labels_tensor_one_hot\\n\\ny_pred = model2.predict(X)\\nprint('y_pred = \\n',y_pred)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model_2(n_hidden, n_chars, n_categories):\n",
    "    model = Sequential()\n",
    "    lstm = LSTM(n_hidden, input_shape=(None,n_chars)) #n_chars = feature size.\n",
    "    model.add(lstm)\n",
    "    model.add(Dense(n_categories, activation='softmax'))\n",
    "    \n",
    "    #initialize hidden states, not sure if the initialization works.\n",
    "    #layer[0] is LSTM\n",
    "    hidden_states = K.variable(value=np.zeros([1, n_hidden]))\n",
    "    cell_states = K.variable(value=np.zeros([1, n_hidden]))\n",
    "    model.layers[0].states[0] = hidden_states\n",
    "    model.layers[0].states[1] = cell_states \n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model2 = build_model_2(n_hidden, n_chars, n_categories)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "X_input = tf.placeholder(tf.float32, shape=(None, None, n_chars))\n",
    "y_output = model2(X_input)\n",
    "\n",
    "X = batch_names_tensor\n",
    "y_true = batch_labels_tensor_one_hot\n",
    "\n",
    "y_pred = model2.predict(X)\n",
    "print('y_pred = \\n',y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print predictions and accuracy comparing with true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = K.get_session()\n",
    "top_n, top_i = tf.nn.top_k(y_pred, k=1)\n",
    "top_values, top_indices = sess.run(tf.nn.top_k(y_pred, k=1))\n",
    "print('y_pred      = ',np.squeeze(top_indices))\n",
    "\n",
    "batch_labels_tensor = np.argmax(batch_labels_tensor_one_hot,axis=1)\n",
    "print('true_labels = ',batch_labels_tensor)\n",
    "\n",
    "#Calculate accuracy of each prediction using Keras metrics\n",
    "metric = tf.keras.metrics.categorical_accuracy(y_true,y_pred)\n",
    "print('Accuracy = ',sess.run(metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_data_generator: num_samples = 20074 num_batches =  2007\n",
      "batch_names_tensor (10, 11, 58)\n",
      "batch_labels_tensor_one_hot (10, 3)\n",
      "\n",
      "batch_labels_tensor_one_hot =\n",
      " [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               95744     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 96,131\n",
      "Trainable params: 96,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "2007/2007 [==============================] - 20s 10ms/step - loss: 1.0940 - acc: 0.3718 0s - loss: 1.0940 - acc: 0.3\n",
      "Epoch 2/20\n",
      "2007/2007 [==============================] - 19s 10ms/step - loss: 1.0760 - acc: 0.4694\n",
      "Epoch 3/20\n",
      "2007/2007 [==============================] - 20s 10ms/step - loss: 1.0631 - acc: 0.4687\n",
      "Epoch 4/20\n",
      "2007/2007 [==============================] - 20s 10ms/step - loss: 1.0534 - acc: 0.4689\n",
      "Epoch 5/20\n",
      "2007/2007 [==============================] - 20s 10ms/step - loss: 1.0463 - acc: 0.4686\n",
      "Epoch 6/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0408 - acc: 0.4686\n",
      "Epoch 7/20\n",
      "2007/2007 [==============================] - 20s 10ms/step - loss: 1.0365 - acc: 0.4687\n",
      "Epoch 8/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0331 - acc: 0.4685\n",
      "Epoch 9/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0301 - acc: 0.4688\n",
      "Epoch 10/20\n",
      "2007/2007 [==============================] - 20s 10ms/step - loss: 1.0278 - acc: 0.4688\n",
      "Epoch 11/20\n",
      "2007/2007 [==============================] - 20s 10ms/step - loss: 1.0258 - acc: 0.4685\n",
      "Epoch 12/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0237 - acc: 0.4688\n",
      "Epoch 13/20\n",
      "2007/2007 [==============================] - 20s 10ms/step - loss: 1.0221 - acc: 0.4688\n",
      "Epoch 14/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0205 - acc: 0.4686\n",
      "Epoch 15/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0188 - acc: 0.4687\n",
      "Epoch 16/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0175 - acc: 0.4685\n",
      "Epoch 17/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0159 - acc: 0.4687\n",
      "Epoch 18/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0146 - acc: 0.4688\n",
      "Epoch 19/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0131 - acc: 0.4688\n",
      "Epoch 20/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 1.0117 - acc: 0.4687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6cd015ae48>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 128\n",
    "batch_size = 10\n",
    "shuffle = True\n",
    "\n",
    "batch_generator = batch_data_generator(all_categories, category_names_dict, batch_size, n_chars, pad_char, shuffle)\n",
    "batch_names_tensor, batch_labels_tensor_one_hot = next(batch_generator)\n",
    "\n",
    "print(\"batch_names_tensor\",batch_names_tensor.shape)\n",
    "print(\"batch_labels_tensor_one_hot\",batch_labels_tensor_one_hot.shape)\n",
    "\n",
    "print(\"\\nbatch_labels_tensor_one_hot =\\n\",batch_labels_tensor_one_hot)\n",
    "\n",
    "\n",
    "my_opt = SGD(lr = 0.0001)\n",
    "\n",
    "mmodel = build_model_2(n_hidden, n_chars, n_categories)    \n",
    "mmodel.compile(loss='categorical_crossentropy', optimizer=my_opt, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "mmodel.fit_generator(\n",
    "          generator=batch_generator,\n",
    "          steps_per_epoch= 20074 // batch_size,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on a batch of training data just to see the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = \n",
      " [[0.47419205 0.17760581 0.34820217]\n",
      " [0.47768733 0.1875687  0.33474395]\n",
      " [0.46545392 0.18263082 0.3519153 ]\n",
      " [0.46246004 0.18976393 0.34777606]\n",
      " [0.45360634 0.19432098 0.3520727 ]\n",
      " [0.44449282 0.18437581 0.3711314 ]\n",
      " [0.4448989  0.1782757  0.37682545]\n",
      " [0.46446285 0.18748379 0.3480533 ]\n",
      " [0.46217299 0.18325418 0.35457283]\n",
      " [0.45085272 0.18284613 0.36630115]]\n",
      "y_pred      =  [0 0 0 0 0 0 0 0 0 0]\n",
      "true_labels =  [2 0 1 1 2 2 2 2 2 2]\n",
      "Accuracy =  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "batch_names_tensor, batch_labels_tensor_one_hot = next(batch_generator)\n",
    "X = batch_names_tensor\n",
    "y_true = batch_labels_tensor_one_hot\n",
    "\n",
    "y_pred = mmodel.predict(X)\n",
    "print('y_pred = \\n',y_pred)\n",
    "\n",
    "sess = K.get_session()\n",
    "top_n, top_i = tf.nn.top_k(y_pred, k=1)\n",
    "top_values, top_indices = sess.run(tf.nn.top_k(y_pred, k=1))\n",
    "print('y_pred      = ',np.squeeze(top_indices))\n",
    "\n",
    "batch_labels_tensor = np.argmax(batch_labels_tensor_one_hot,axis=1)\n",
    "print('true_labels = ',batch_labels_tensor)\n",
    "\n",
    "#Calculate accuracy of each prediction using Keras metrics\n",
    "metric = tf.keras.metrics.categorical_accuracy(y_true,y_pred)\n",
    "print('Accuracy = ',sess.run(metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               95744     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 96,131\n",
      "Trainable params: 96,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 0.8018 - acc: 0.6484\n",
      "Epoch 2/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 0.6131 - acc: 0.7387\n",
      "Epoch 3/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 0.5453 - acc: 0.7742\n",
      "Epoch 4/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 0.5141 - acc: 0.7872\n",
      "Epoch 5/20\n",
      "2007/2007 [==============================] - 21s 11ms/step - loss: 0.4953 - acc: 0.7951\n",
      "Epoch 6/20\n",
      "2007/2007 [==============================] - 21s 11ms/step - loss: 0.4805 - acc: 0.8027\n",
      "Epoch 7/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 0.4715 - acc: 0.8071\n",
      "Epoch 8/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 0.4620 - acc: 0.8109\n",
      "Epoch 9/20\n",
      "2007/2007 [==============================] - 21s 11ms/step - loss: 0.4542 - acc: 0.8133\n",
      "Epoch 10/20\n",
      "2007/2007 [==============================] - 21s 10ms/step - loss: 0.4493 - acc: 0.8156\n",
      "Epoch 11/20\n",
      "2007/2007 [==============================] - 21s 11ms/step - loss: 0.4441 - acc: 0.8187\n",
      "Epoch 12/20\n",
      "2007/2007 [==============================] - 21s 11ms/step - loss: 0.4392 - acc: 0.8209\n",
      "Epoch 13/20\n",
      "2007/2007 [==============================] - 22s 11ms/step - loss: 0.4338 - acc: 0.8233\n",
      "Epoch 14/20\n",
      "2007/2007 [==============================] - 22s 11ms/step - loss: 0.4316 - acc: 0.8238\n",
      "Epoch 15/20\n",
      "2007/2007 [==============================] - 24s 12ms/step - loss: 0.4265 - acc: 0.8275\n",
      "Epoch 16/20\n",
      "2007/2007 [==============================] - 22s 11ms/step - loss: 0.4230 - acc: 0.8289\n",
      "Epoch 17/20\n",
      "2007/2007 [==============================] - 22s 11ms/step - loss: 0.4188 - acc: 0.8285\n",
      "Epoch 18/20\n",
      "2007/2007 [==============================] - 22s 11ms/step - loss: 0.4165 - acc: 0.8316\n",
      "Epoch 19/20\n",
      "2007/2007 [==============================] - 22s 11ms/step - loss: 0.4129 - acc: 0.8314\n",
      "Epoch 20/20\n",
      "2007/2007 [==============================] - 22s 11ms/step - loss: 0.4080 - acc: 0.8337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6cc821ea20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_opt = Nadam(lr=0.0001,\n",
    "              beta_1=0.9,\n",
    "              beta_2=0.999,\n",
    "              epsilon=1e-8,\n",
    "              schedule_decay=0.004)\n",
    "\n",
    "mmodel = build_model_2(n_hidden, n_chars, n_categories)    \n",
    "mmodel.compile(loss='categorical_crossentropy', optimizer=my_opt, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "mmodel.fit_generator(\n",
    "          generator=batch_generator,\n",
    "          steps_per_epoch= 20074 // batch_size,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on a batch of training data just to see the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred = \n",
      " [[5.34338713e-01 1.45338097e-04 4.65515912e-01]\n",
      " [6.53709888e-01 1.18592076e-01 2.27698028e-01]\n",
      " [9.99956131e-01 4.31557010e-06 3.95785100e-05]\n",
      " [8.01142305e-02 5.97447157e-01 3.22438627e-01]\n",
      " [3.10353450e-02 2.87276834e-01 6.81687832e-01]\n",
      " [9.99530435e-01 3.51038325e-04 1.18538788e-04]\n",
      " [1.15735158e-01 3.28714401e-02 8.51393402e-01]\n",
      " [9.99712646e-01 8.49747448e-06 2.78907421e-04]\n",
      " [9.97492909e-01 2.08451020e-04 2.29862332e-03]\n",
      " [9.99835610e-01 5.13850682e-05 1.12933209e-04]]\n",
      "y_pred      =  [0 0 0 1 2 0 2 0 0 0]\n",
      "true_labels =  [0 0 0 1 1 0 2 0 0 0]\n",
      "Accuracy =  [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "batch_names_tensor, batch_labels_tensor_one_hot = next(batch_generator)\n",
    "X = batch_names_tensor\n",
    "y_true = batch_labels_tensor_one_hot\n",
    "\n",
    "y_pred = mmodel.predict(X)\n",
    "print('y_pred = \\n',y_pred)\n",
    "\n",
    "sess = K.get_session()\n",
    "top_n, top_i = tf.nn.top_k(y_pred, k=1)\n",
    "top_values, top_indices = sess.run(tf.nn.top_k(y_pred, k=1))\n",
    "print('y_pred      = ',np.squeeze(top_indices))\n",
    "\n",
    "batch_labels_tensor = np.argmax(batch_labels_tensor_one_hot,axis=1)\n",
    "print('true_labels = ',batch_labels_tensor)\n",
    "\n",
    "#Calculate accuracy of each prediction using Keras metrics\n",
    "metric = tf.keras.metrics.categorical_accuracy(y_true,y_pred)\n",
    "print('Accuracy = ',sess.run(metric))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
